# 第2周：卷积神经网络



## 引言

本周学习视频为“02-卷积神经网络”，下载链接为：https://www.jianguoyun.com/p/Dde3HS8QrKKIBhi2xpEGIAA



<br>


## 1、视频学习

学习视频：卷积神经网络，主要内容包括：

- CNN的基本结构：卷积、池化、全连接
- 典型的⽹络结构：AlexNet、VGG、GoogleNet、ResNet

<br>

## 2、代码练习

###  实验3：MNIST数据集分类

构建简单的CNN对 mnist 数据集进⾏分类。同时，还会在实验中学习池化与卷积操作的基本作⽤， 实验指导链接：[实验3： 使用CNN对MINIST数据集分类](https://oucaigroup.feishu.cn/wiki/ZZHiwlpZJiLuNCkpL5ScHeCTnqg)

要求：把代码输入 colab，在线运行观察效果。

### 实验4：CIFAR10 数据集分类

使⽤ CNN 对 CIFAR10 数据集进⾏分类， 实验指导链接：[实验4：使用LeNet对CIFAR10数据分类](https://oucaigroup.feishu.cn/wiki/TWFdw1ecwiBILikSJrecvMX4nOf)

要求：把代码输入 colab，在线运行观察效果。

### 实验5：VGG16对CIFAR10分类

- 使⽤ VGG16 对 CIFAR10 分类，实验指导链接：[实验5：使用VGG对CIFAR10分类](https://oucaigroup.feishu.cn/wiki/UJ1xwmjVziixTckP1zFcgNgcnqc)

要求：把代码输入 colab，在线运行观察效果。

<br>



## 3、博客作业

完成一篇博客，思考下面的问题：
- dataloader 里面 shuffle 取不同值有什么区别？
- transform 里，取了不同值，这个有什么区别？
- epoch 和 batch 的区别？
- 1x1的卷积和 FC 有什么区别？主要起什么作⽤？
- residual leanring 为什么能够提升准确率？
- 代码练习二里，网络和1989年 Lecun 提出的 LeNet 有什么区别？
- 代码练习二里，卷积以后feature map 尺寸会变⼩，如何应⽤ Residual Learning?
- 有什么方法可以进⼀步提升准确率？
如果还有其它问题，可以总结一下，写在博客里，下周一起讨论。

<br>

## 4、实验环节

### 4.1 pytorch 基础练习


